# Listen，Attend，and Spell（LAS）

LAS即传统的**sequence-to-sequence**方法。

其中包含了：编码器Encoder、解码器Decoder

第一篇使用sequence-to-sequence方法取得了非常成功效果的文章来自2015年，是在传统方法上引入了**注意力机制ATTENTION**。

题外话：当年的学术界比较流行用三个词组作为文章标题的方法

## Listen

理想状态下，模型希望输出部分只继承输入部分的有效特征。

##  Listen——Down Sampling

一段表示为Acoustic Feature的声音信号往往都太长了，如果节省计算量，提高效率，就需要采用Down Sampling的方式进行**下采样**的声音信号处理

目的：减少RNN的计算量

## Attention

## Spell

## Beam Search

树模型和贪心算法

在实际上没有办法遍历所有的路径去寻找真正最好的那个人生轨迹，所以贪心只能算作是一种理论上尽可能收益最大化的方式。（这个时候，李宏毅老师举了一个签不签读博班的例子）

## Training

## Why Teacher Forcing

一种专注于训练含有Decoder的sequence-to-sequence模型的方法。让算法专注于正确的结果学习序列前的信息

## Back to Attention

拥有Attention的sequence-to-sequence最早是用在翻译任务上的。

在翻译任务上，输入和输出没有位置上的一一对应关系，因此就需要通过Attention的方式让Decoder从Encoder上去寻找重要的关键信息。

但是，这个方法在声音信号上是行不通的，也是不符合逻辑的。

因为声音是连续的序列信息，先输出的声音，就会被先听到，这是理所当然的结果。

## LAS-Does it work？

一系列的论文实验结果显示：

小数据集上的训练不起作用，结果甚至不如传统方法的结果。

但随着数据集的时长增加，识别的正确率显著提升

## Hokkien（闽南语、台语）

小语种以及方言的语音识别，在实现思路上其实还是“硬Train一发”。

方法：用爬虫在视频网站上怕取大量数据，然后直接用LAS训练下去

（有大量的杂音，音效，语音文本未对齐等结果怎么办……依然“硬Train一发！”）

## Limitation of LAS

如今的需求是更快更好的将语音信号进行实时的输出。

而这一点，LAS没有办法做到，接下来的模型可以解决这个问题。