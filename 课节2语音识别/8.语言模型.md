# Language Modeling For Speech Recognition

这一部分将讲述：如何将语言模型放到语音辨识任务中去

## 为什么需要LM

$$
\mathrm{Y}^{*}=\arg \max _{\mathrm{Y}} P(\mathrm{Y} \mid X) P(Y)
$$

在上面公式中，
$$
P(\mathrm{Y} \mid X)
$$
表示训练数据集中,成对儿出现的训练数据(Y是文字，X是语音)。
$$
P(Y)
$$
表示在训练数据中，单纯是文字的训练数据。

**从公式上分析**：既然二者都和最终效果呈正相关。在前者的数据需要大量的人工标注且数据稀少的情况下，大量提高后者的数量和正确率才是提高模型效率的最好方法。

那么后者对应的**语言模型Language Model**就非常关键了。



结果表明：<u>非常有用。</u>

往往在需要用到文字和语言的地方，只要加上了Language Model就是会提升效果，就是会起作用。

### BERT（重点）

BERT：一个巨大的LM，其中使用了超过30亿个单词。

### N-gram

通过token sequence的数量来定义N-gram中的N，最终扩展为2-gram、3-gram、4-gram……

这样的好处是：可以明确的将上下文的信息建立关联权重。

缺点是：如果数据不够多，将会认为上下文没有关联的数据完全没有关系（不能保证两个没见过面的数据是完全没有关系的）

因此通过将所有数据都关联都设置为0.0001的方法，使数据都有小小的关联。这种方法称为：**Language Model Smoothing**

### Continuous LM

#### Matrix Factorization

在进入深度学习时代之前，是没有**Language Model Smoothing**方法的。

因此，从推荐系统中借用了Matrix Factorization方法。一种通过分析用户使用率的方法去预估用户喜好的表格内容估算方法。



在推荐系统中，横轴和纵轴分别是用户的喜爱程度和用户喜欢的物品。

当模型迁移到大语言模型之后，横轴和纵轴都是**单词**，二者相交内容的数值代表了**两个单词连续出现的统计次数**。

这样的方法本质上还是**统计学**的方法，而非深度学习的方法。

### NN-based LM

早在这篇文章发表的2003年，就已经在文章中提出过word embedding。

### RNN-based LM

将关联词语变成句子/文章……（越来越长）

### How to use LM to improve LAS?



# 总结

三种 Language model + LAS 的方法