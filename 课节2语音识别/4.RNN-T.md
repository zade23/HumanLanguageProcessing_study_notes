# RNN-T

## RNA：Recurrent Neural Aligner（2017）

- 解决CTC不能做到的输入输出的一对多的实现
-  φ代表“请输入下一个输入”，"_"代表了空格

和CTC类似的是，RNA也会穷举所有的可能的输入形式，才会知道到底输出对应了哪一种输入。

## LM：Language Model

在RNN中为了训练而设计的一种方法。忽视语音，仅重视词向量token

# Neural Transducer

每一次都读取N个attention的输入。

每一次凑够了N个数据才作为输入进行计算。这是一种类似滑窗模型的思路。

这种方法在引入了Attention方法以后，效果还是不错的。

# MoChA：Monotonic Chunkwise Attention

“动态移动网络的数据滑窗”：自由的决定每次做滑窗的距离是多少。

每一个数据输入后都进行一次判断，判断是否作为本次滑窗的终点。

（这个方法有意思的地方在于：决定某个数据是否作为滑窗终点的判断条件是一种二分类判断，到底做不做为终点，详情在论文会有说明）

# 总结

LAS：就是seq2seq

CTC：Decoder是Linear Classifier的seq2seq

RNA：输入一个就输出一个的seq2seq

RNN-T：输入一个东西可以输出多个东西的seq2seq

Neural Transducer：每次输入一个window的RNN-T

MoCha：window移动伸缩自如的Neural Transducer

