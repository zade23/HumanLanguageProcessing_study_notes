# Voice Conversion

## 语音转换（VC）是什么（What is Voice Conversion）

语音转换的任务实际上就是：输入一段声音，再输出一段声音即可。

这两段声音，我们希望有一些部分是**被保留**的，有一些部分是**改变**的。

### 保留部分

- 声音的内容

### 改变部分

随任务目的的不同而改变。

#### Speaker——改变说话人的声音

- 同样一句话，用另外一个人的声音说出来（柯南的领结变声器）
- Deep Fake：仿照一个人的声音，甚至是相貌
- 制作一个个人化语音系统
- 歌声转换
- 消除隐私（一个人的声音会透露出非常多的信息，例如年龄、性别、健康状况）

#### 说话风格（Speaking Style）

- 情绪化声音（Emotion）
- 给原始说话声音添加环境音（Normal-to-Lombard）：给予原始说话人音频一个环境声音(例如：酒吧、舞会)
- 悄悄话转正常说话（Whisper-to-Normal）：声音放大
- 歌唱技巧美化（弹舌、颤音）

#### 提高可解释性（Improving Intelligibility）

- 帮助发音障碍者修复说话语音的连贯性
- 口音的转换

#### 数据扩增（Data Augmentation）

- 数据扩增：用生成的声音作为数据集，重新加入训练资料中

## 实际应用场景（In Real Implementation）

一些在训练和实践过程中的例子：

1. 输入和输出的长度可以不同

如果长度不相同，二者就必须使用seq2seq的模型才可以进行训练。

但是，如果二者的长度是相同的，那么就可以不用seq2seq的模型来进行训练，因为可以使用更简单的方法去实现长度不相同时模型的训练。

未来发展的方向会往输入和输出不相同的方向前进（未来模仿更真实想韵律，长度上有所变化才是重中之重吧）

注意：

频谱转换到声音信号的过程中间，还需要**相位差**。

### 声码器（Vocoder）

**声码器**将**声学特征(acoustic feature)**作为输入，最终输出声音信号。

因为这一功能，将Vocoder称之为**声码器**。

这一步使用到的规则范式：

**Rule-based：Griffin-Lim algorithm**

**Deep Learning：WaveNet**

## 按照数据集形式将训练分类

### 以标注的对应数据（Parallel Data）

输入输出人物所说的话是同一句话，数据集中二者是成对出现的。

而这样“完美”的数据，是非常难获取的。

### 单音频数据（Unparallel Data）

最常见也是最常用的一种方法。

数据往往都是不对称的，而且这样的数据是大量存在的。在图片风格转换的任务中，**风格转换style transfer**的任务广泛存在，那么应用到音频的风格转换上也是可以实现的。

![](.\picture\2.png)

#### 特征分离（Feature Disentangle）

**<u>说在开头：做特征分离，其实就类似在做一个自编码（Auto encoder）的网络架构</u>**

分离一句话中的特征。

将说话人的声音和说话内容进行分割，只改变说话人声音，不改变内容(这个在理解上有些许的抽象)

![将李宏毅老师或的一段话进行内容分割](.\picture\3.png)

如上图所示的，将李宏毅老师的一段声音信号用**编码器（Encoder）**进行分割为：

- 内容资讯（Content）
- 语者资讯（Speaker）

将分割后的结果进行整理合并，最终送入**解码器（Decoder）**中，最终就可以用语者的声音念出这段话的内容。

##### Tips：利用特征分割能做什么

答：将**说话人（Speaker）**特征进行替换以后，就可以实现**语音转换（Voice Conversion）**的变声器功能。

![](.\picture\4.png)

比如，通过Speaker Encoder的声音信号替换成新垣结衣的声音，将Content和Speaker整合交给Decoder，最终就可以合成用新垣结衣声音念出“Do you want to study PhD？”这句话了。（道理是这个道理，但是实际效果上还有最终听觉感受上最终还有很长的调整和优化过程）

##### 利用说话人信息（Using Speaker Information）

27.44

优点：可行且高效。

缺点：没有办法合成新的语音的声音。One-hot信号训练的方式下，如果有新的声音加入训练集，就必须重新训练这个模型

##### Pre-Training Encoder

##### Adversarial Training

##### Designing network architecture

![](.\picture\1.png)

##### 声音分离部分知识点总结

通过Auto Decoder产生的声音信号都不是非常的理想。

我们想要得到的是用A的声音说B的内容，结果往往都是事与愿违的。因为在最终的结果中，之前的训练一直都是进行声音的分离和合成都是用同一个人的声音，但是如果在测试阶段用到了不同的人物声音，那么对于模型来说，可能会导致学出来的声音是非常低质量的声音。

因此，就有下一步的进步。

两阶段的训练模型。

##### 分成两阶段的模型训练（2nd Stage Training）

解决办法是：<u>在训练阶段，就把**语音切换（Voice Conversion）**这件事情考虑进来。</u>

两阶段训练的方法是将同样的内容用不同的说话人声音进行训练，这样让模型更好的知晓和区分出说话人和说话内容上的差别。

但这样做产生出的问题是：<u>这样的训练方法并没有**基线数据（grounding truth）**作为正确答案的参考，没有办法做训练。</u>

如果没有基线数据来进行训练的方法是有的，**GAN模型**就是通过对抗学习的方法训练并制作伪造数据的。

通过加入一个<u>**判别器**</u>，从而对生成的数据进行判断，判断其到底来真实数据还是合成数据。

进一步的，还可以添加一个发音人物分类器，用于区分说话人是否是同一个。

缺点：**容易训练失败。**GAN网络本身就具有不容易训练和难收敛的问题，通过生成对抗的方式进行训练，很容易在开始就训练不起来。

解决方案：**加一个Patcher补丁。**通过Patcher补充的声音加和，通过训练另一个（未完待续）

#### 直接转换（Direct Transformation）

这种方案下有几个比较经典的模型网络

##### CycleGAN

直接转换任务中，可以使用Cycle GAN这一技术栈。

和图像生成部分的方法一模一样，只是将这部分的东西转化到了音频的生成上。

从实现架构上讲，非常的像**“硬Train一发”**的方法，就是通过网络结构将输入数据合成为目标输出。

这个合成出来的数据会再和真实的目标声音一起送入一个判别器，这个判别器用于判断生成的这个声音到底是不是目标人物发出的声音。

具体流程方法如下图所示

![企业微信截图_16777458108583](.\picture\企业微信截图_16777458108583.png)

下一个问题：**<u>如何让两段声音信号，越接近越好？</u>**

引出下一个技术栈：**循环一致性（Cycle Consistency）**

声音信号的本质处理方式是将一段信号变成一段**语音特征序列（向量序列）**放入**神经网络**中去，再通过网络，将这个内容输出为一段**语音特征序列（向量序列）**。最后在通过一个生成器将目标人物的声音转换回原始说话人的声音，通过**L1正则**或**L2正则**来判断最终是否和原始声音相同，从而形成了一个循环（这也是网络取名为CycleGAN的由来）。完整的网络流程如下

![企业微信截图_16777470829841](.\picture\企业微信截图_16777470829841.png)

##### StarGAN

StarGAN是CycleGAN的进阶版。

CycleGAN固然好用，且可以解决训练上的问题。但由于其本质是在数据声音和目标声音之间进行一对一的模型训练，当假设有四个人物声音需要训练合成，那么效率就会非常低下。

（假如你有N个说话人，那么你序号N*(N - 1)个生成器才能生成对应的说话人转目标人物的任务）

![企业微信截图_16777470829841](.\picture\企业微信截图_167774777514.png)



# 学习过程中的疑问

## 1.Decoder和vocoder二者是否相同？如果不同，有什么区别

解释：

在语音合成领域，Decoder和Vocoder不是同一个东西，它们有不同的功能和应用。

### Decoder

Decoder通常指的是语音合成模型中的一部分，它的主要功能是将输入的语音文本转换为语音信号。具体来说，Decoder通常采用端到端的模型，将语音文本作为输入，经过一系列神经网络模块的处理，输出语音信号。Decoder的设计通常需要考虑如何提取文本信息、如何建模声学特征、如何控制声音的流畅度和自然度等问题。

### Vocoder

Vocoder则是语音信号合成中的一个关键组成部分，其作用是将语音信号转换为数字信号，便于数字信号的处理和传输。Vocoder的主要任务是将数字信号转换为声音信号，通常采用信号处理方法，例如频域分析、声道模型等，可以实现高质量、高效的语音信号合成。Vocoder的设计需要考虑如何准确重构语音信号、如何保持语音信号的质量和自然度等问题。

因此，虽然Decoder和Vocoder都与语音合成相关，但它们的功能和应用是不同的，二者是两个独立的组件。在语音合成系统中，Decoder和Vocoder通常需要配合使用，通过对文本和语音信号的处理，实现高质量、自然度的语音合成效果。



## 2.GAN模型和Diffusion模型在定义和功能实现上有哪些异同

解释：

GAN（生成对抗网络）和Diffusion（扩散模型）是两种不同的生成模型，在定义和实现上有一些相同点和不同点。

### 相同点

1. 两种模型都可以用于生成高质量的图像、视频、音频等数据。

2. 两种模型都采用生成模型的思想，即学习数据的分布并生成新的数据。

3. 两种模型都需要训练数据集，并根据训练数据集的统计信息来生成新的数据。


### 不同点

1. GAN是一种基于对抗训练的生成模型，它的训练过程是通过两个网络之间的对抗学习来实现的。其中一个网络（生成器）生成虚假数据，另一个网络（判别器）评估生成的数据的真实程度。GAN的训练过程比较复杂，需要平衡生成器和判别器之间的训练效果。

2. Diffusion是一种基于扩散过程的生成模型，它的核心思想是将噪声信号逐渐扩散到真实数据的分布。Diffusion的训练过程比较简单，可以通过多次迭代的方式逐渐调整模型参数，使得模型可以生成接近真实数据分布的样本。

3. GAN通常需要较长的训练时间，需要更大的计算资源和更高的技术要求。而Diffusion的训练时间相对较短，可以在较小的计算资源下实现。

4. GAN生成的图像通常具有更好的视觉效果，可以实现更加逼真的图像合成。而Diffusion生成的图像则更加注重数据的准确性和可控性，可以实现更加高质量的数据重建。


综上所述，GAN和Diffusion都是生成模型的一种，二者在定义和实现上有相同点和不同点，具体选择哪种模型取决于具体应用场景和需求。



## 3.回顾L1正则化和L2正则化的定义以及二者的区别

解释：

L1正则化和L2正则化都是机器学习中常用的正则化方法，用于**<u>控制模型的复杂度和防止过拟合</u>**。

### L1正则化

L1正则化是指在模型损失函数中加入L1范数的惩罚项，用于控制模型权重的大小。具体来说，L1正则化将模型权重的绝对值之和作为惩罚项，即：

$$
λ * Σ |w|
$$
其中，w表示模型的权重，λ表示正则化参数，用于调整正则化强度。L1正则化倾向于将一些不重要的特征的权重设为0，从而达到特征选择的效果。

### L2正则化

L2正则化是指在模型损失函数中加入L2范数的惩罚项，用于控制模型权重的大小。具体来说，L2正则化将模型权重的平方和作为惩罚项，即：

$$
λ * Σ w^2
$$
其中，w表示模型的权重，λ表示正则化参数，用于调整正则化强度。L2正则化可以使得模型的权重尽量接近0，从而使得模型更加平滑，防止过拟合。

### L1正则化和L2正则化的区别

L1正则化和L2正则化的主要区别在于惩罚项的不同。L1正则化倾向于将一些不重要的特征的权重设为0，从而实现特征选择的效果，而L2正则化可以使得模型的权重更加平滑，防止过拟合。<u>此外，L1正则化产生的稀疏权重向量使得模型更容易解释，而L2正则化在一些情况下会导致权重向量中的所有权重都接近0，这可能使得模型的解释性变得困难。</u>

在实践中，通常需要通过交叉验证等方法来选择合适的正则化参数λ，以达到最佳的模型效果。
